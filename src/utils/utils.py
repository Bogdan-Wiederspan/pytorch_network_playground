from __future__ import annotations

from collections import defaultdict
from collections.abc import Iterable

from copy import deepcopy

EMPTY_INT = -99999
EMPTY_FLOAT = -99999.0

import torch
import numpy as np
import awkward as ak
import onnx
import onnxruntime as rt


# def expand_columns(columns: Container[str]) -> list[Route]:
#     final_set = set()
#     final_set.update(*list(map(Route, law.util.brace_expand(obj)) for obj in columns))
#     return sorted(final_set, key=str)

def normalized_weight_decay(
    model: torch.nn.Module,
    decay_factor: float = 1e-1,
    normalize: bool = True,
    apply_to: str = "weight",
) -> tuple[dict, dict]:
    """
    Weight decay should only be applied to the linear layers or convolutional layers.
    All other layers should not have weight decay applied.
    Pytorch Optimizers will apply weight decay to all parameters that have `requires_grad=True`.
    This function can be overwritten by specify the parameters that should have weight decay applied.

    Args:
        model (torch.nn.Module): The model to apply weight decay to.
        decay_factor (float): The weight decay factor to apply to the parameters.
        normalize (bool): If True, the decay factor is normalized by the number of parameters.
            This ensures that the end L2 loss is about the same size for big and small models.

    Returns:

        tuple: A tuple containing two dictionaries where:
            - The first dictionary contains parameters that should not have weight decay applied.
            - The second dictionary contains parameters that should have weight decay applied.
    """
    # get list of parameters that should have weight decay applied, and those that should not
    with_weight_decay = []
    no_weight_decay = []
    # only add weight decay to linear layers! everything else should run normally.
    for module_name, module in model.named_modules():
        # get only modules that are not containers
        if len(list(module.named_modules())) == 1:
            for parameter_name, parameter in module.named_parameters():
                if (isinstance(module, torch.nn.Linear)) and (apply_to in parameter_name):
                    with_weight_decay.append(parameter)
                    print(f"add weight decay to: module:{module}//named: {module_name}// paramter:{parameter_name}")
                else:
                    no_weight_decay.append(parameter)

    # decouple lambda choice from number of parameters, by normalizing the decay factor
    num_weight_decay_params = sum([len(weight.flatten()) for weight in with_weight_decay])
    if normalize:
        decay_factor = decay_factor / num_weight_decay_params
        print(f"Normalize weight decay by number of parameters: {decay_factor}")
    return {"params": no_weight_decay, "weight_decay": 0.0}, {"params": with_weight_decay, "weight_decay": decay_factor}

# embedding_expected_inputs = {
#     "pair_type": [0, 1, 2],  # see mapping below
#     "decay_mode1": [-1, 0, 1, 10, 11],  # -1 for e/mu
#     "decay_mode2": [0, 1, 10, 11],
#     "lepton1.charge": [-1, 1, 0],
#     "lepton2.charge": [-1, 1, 0],
#     "has_fatjet": [0, 1],  # whether a selected fatjet is present
#     "has_jet_pair": [0, 1],  # whether two or more jets are present
#     # 0: 2016APV, 1: 2016, 2: 2017, 3: 2018, 4: 2022preEE, 5: 2022postEE, 6: 2023pre, 7: 2023post
#     "year_flag": [0, 1, 2, 3, 4, 5, 6, 7],
#     "channel_id": [1, 2, 3],
# }

def clip_gradients(parameters: Iterable[torch.nn.Parameter], clip_value: float = 1.0):
    for p in parameters:
        p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))



def export_ensemble_onnx(
    ensemble_wrapper,
    categoricat_tensor: torch.Tensor,
    continous_tensor: torch.Tensor,
    save_dir: str,
    opset_version: int = None,
) -> str:
    """
    Wrapper to export an ensemble model to onnx format. Does the same as export_onnx, but
    iterates over all models in the ensemble_wrapper and freezes them before exporting.

    Args:
        ensemble_wrapper (MLEnsembleWrapper): _description_
        categoricat_tensor (torch.tensor): tensor representing categorical features
        continous_tensor (torch.tensor): tensor representing categorical features
        save_dir (str): directory where the onnx model will be saved.
        opset_version (int, optional): version of the used operation sets. Defaults to None.

    Returns:
        str: The path of the saved onnx ensemble model.
    """
    for model in ensemble_wrapper.models:
        model.eval()
        for param in model.parameters():
            param.requires_grad = False

    export_onnx(
        ensemble_wrapper,
        categoricat_tensor,
        continous_tensor,
        save_dir,
        opset_version=opset_version,
    )


def export_onnx(
    model: torch.nn.Module,
    categoricat_tensor: torch.Tensor,
    continous_tensor: torch.Tensor,
    save_dir: str,
    opset_version: int = None,
) -> str:
    """
    Function to export a loaded pytorch *model* to onnx format saved in *save_dir*.
    To successfully export the model, the input tensors *categoricat_tensor* and *continous_tensor* must be provided.
    For backwards compatibility, an opset_version can be enforced.
    A table about which opsets are available can be found here: https://onnxruntime.ai/docs/reference/compatibility.html
    Some operations are only available in newer opsets, or change behavior inbetween version.
    A list of all operations and their versions is given at: https://onnx.ai/onnx/operators/

    Args:
        model (torch.nn.model): loaded Pytorch model, ready to perform inference.
        categoricat_tensor (torch.tensor): tensor representing categorical features
        continous_tensor (torch.tensor): tensor representing categorical features
        save_dir (str): directory where the onnx model will be saved.
        opset_version (int, optional): version of the used operation sets. Defaults to None.

    Returns:
        str: The path of the saved onnx model.
    """

    # logger = law.logger.get_logger(__name__)

    onnx_version = onnx.__version__
    runtime_version = rt.__version__
    torch_version = torch.__version__

    save_path = f"{save_dir}-onnx_{onnx_version}-rt_{runtime_version}-torch{torch_version}.onnx"

    # prepare export
    num_cat_features = categoricat_tensor.shape[-1]
    num_cont_features = continous_tensor.shape[-1]

    # cast to proper format, numpy and float32
    categoricat_tensor = categoricat_tensor.numpy().astype(np.float32).reshape(-1, num_cat_features)
    continous_tensor = continous_tensor.numpy().astype(np.float32).reshape(-1, num_cont_features)

    # double bracket is necessary since onnx, and our model unpacks the input tuple
    input_feed = ((categoricat_tensor, continous_tensor),)

    torch.onnx.export(
        model,
        input_feed,
        save_path,
        input_names=["cat", "cont"],
        output_names=["output"],
        # if opset is none highest available will be used

        opset_version=opset_version,
        do_constant_folding=True,
        dynamic_axes={
            # enable dynamic batch sizes
            "cat": {0: "batch_size"},
            "cont": {0: "batch_size"},
            "output": {0: "batch_size"},
        },
    )

    # logger.info(f"Succefully exported onnx model to {save_path}")
    return save_path


def test_run_onnx(
    model_path: str,
    categorical_array: np.ndarray,
    continous_array: np.ndarray,
) -> np.ndarray:
    """
    Function to run a test inference on a given *model_path*.
    The *categorical_array* and *continous_array* are expected to be given as numpy arrays.

    Args:
        model_path (str): Model path to onnx model
        categorical_array (np.ndarray): Array of categorical features
        continous_array (np.ndarray): Array of continous features

    Returns:
        np.ndarray: Prediction of the model
    """
    sess = rt.InferenceSession(model_path, providers=rt.get_available_providers())
    first_node = sess.get_inputs()[0]
    second_node = sess.get_inputs()[1]

    # setup data
    input_feed = {
        first_node.name: categorical_array.reshape(-1, first_node.shape).astype(np.float32),
        second_node.name: continous_array.reshape(-1, second_node.shape).astype(np.float32),
    }

    output_name = [output.name for output in sess.get_outputs()]

    onnx_predition = sess.run(output_name, input_feed)
    return onnx_predition


def get_standardization_parameter(
    data_map: list[ParquetDataset],
    columns: Iterable[Route | str],
) -> dict[str, ak.Array]:
    # open parquet files and concatenate to get statistics for whole datasets
    # beware missing values are currently ignored
    all_data = ak.concatenate(list(map(lambda x: x.data, data_map)))
    # make sure columns are Routes
    columns = list(map(Route, columns))

    statistics = {}
    for _route in columns:
        # ignore empty fields
        arr = _route.apply(all_data)
        # filter missing values out
        empty_mask = arr == EMPTY_FLOAT
        masked_arr = arr[~empty_mask]
        std = ak.std(masked_arr, axis=None)
        mean = ak.mean(masked_arr, axis=None)
        # reshape to 1D array, torch has no interface for 0D
        statistics[_route.column] = {"std": std.reshape(1), "mean": mean.reshape(1)}
    return statistics


def expand_columns(*columns):
    if isinstance(columns, str):
        columns = [columns]

    _columns = set()
    for column_expression in columns:
        if isinstance(column_expression, Route):
            # do nothing if already a route
            break
        expanded_columns = law.util.brace_expand(column_expression)
        routed_columns = set(map(Route, expanded_columns))
        _columns.update(routed_columns)
    return sorted(_columns)


def reorganize_list_idx(entries):
    first = entries[0]
    if isinstance(first, int):
        return entries
    elif isinstance(first, dict):
        return reorganize_dict_idx(entries)
    elif isinstance(first, (list, tuple)):
        sub_dict = defaultdict(list)
        for e in entries:
            # only the last entry is the idx, all other entries
            # in the list/tuple will be used as keys
            data = e[-1]
            key = tuple(e[:-1])
            if isinstance(data, (list, tuple)):
                sub_dict[key].extend(data)
            else:
                sub_dict[key].append(e[-1])
        return sub_dict


def reorganize_dict_idx(batch):
    return_dict = dict()
    for key, entries in batch.items():
        # type shouldn't change within one set of entries,
        # so just check first
        return_dict[key] = reorganize_list_idx(entries)
    return return_dict


def reorganize_idx(batch):
    if isinstance(batch, dict):
        return reorganize_dict_idx(batch)
    else:
        return reorganize_list_idx(batch)



def LookUpTable(array: torch.Tensor, EMPTY=EMPTY_INT, placeholder: int = 15):
    """Maps multiple categories given in *array* into a sparse vectoriced lookuptable.
    Empty values are replaced with *EMPTY*.

    Args:
        array (torch.Tensor): 2D array of categories.
        EMPTY (int, optional): Replacement value if empty. Defaults to columnflow EMPTY_INT.

    Returns:
        tuple([torch.Tensor]): Returns minimum and LookUpTable
    """
    # add placeholder value to array
    array = torch.cat([array, torch.ones(array.shape[0], dtype=torch.int32).reshape(-1, 1) * placeholder], axis=-1)
    # shift input by minimum, pushing the categories to the valid indice space
    minimum = array.min(axis=-1).values
    indice_array = array - minimum.reshape(-1, 1)
    upper_bound = torch.max(indice_array) + 1

    # warn for big categories
    if upper_bound > 100:
        print("Be aware that a large number of categories will result in a large sparse lookup array")

    # create mapping placeholder
    mapping_array = torch.full(
        size=(len(minimum), upper_bound),
        fill_value=EMPTY,
        dtype=torch.int32,
    )

    # fill placeholder with vocabulary

    stride = 0
    # transpose from event to feature loop
    for feature_idx, feature in enumerate(indice_array):
        unique = torch.unique(feature, dim=None)
        mapping_array[feature_idx, unique] = torch.arange(
            stride, stride + len(unique),
            dtype=torch.int32,
        )
        stride += len(unique)
    return minimum, mapping_array

class CategoricalTokenizer(torch.nn.Module):
    def __init__(self, translation: torch.Tensor, minimum: torch.Tensor):
        """
        This translaytion layer tokenizes categorical features into a sparse representation.
        The input tensor is a 2D tensor with shape (N, M) where N is the number of events and M of features.
        The output tensor will have shape (N, K) where K is the number of unique categories across all features.

        Args:
            translation (torch.tensor): Sparse representation of the categories, created by LookUpTable.
            minimum (torch.tensor): Array of minimas used to shift the input tensor into the valid index space.
        """
        super().__init__()
        self.map = translation
        self.min = minimum
        self.indices = torch.arange(len(minimum))

    @property
    def num_dim(self):
        return torch.max(self.map) + 1

    def forward(self, x):
        # shift input array by their respective minimum and slice translation accordingly
        return self.map[self.indices, x - self.min]

    def to(self, *args, **kwargs):
        # make sure to move the translation array to the same device as the input
        self.map = self.map.to(*args, **kwargs)
        self.min = self.min.to(*args, **kwargs)
        self.indices = self.indices.to(*args, **kwargs)
        return super().to(*args, **kwargs)
